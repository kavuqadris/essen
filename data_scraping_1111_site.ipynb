{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4fb7d6a"
      },
      "source": [
        "## 1111 Job Listing Scraper\n",
        "\n",
        "This notebook is my practice of scraping job listings on the 1111.com.tw website. The 1111 website was chosen as a target for my practice due to its relatively straightforward structure and apparent lack of resctriction on anti-data mining systems, which makes it suitable for me and illustrating web scraping techniques.\n",
        "\n",
        "The HTML class structures on the 1111 website are also clear and consistent, simplifying the process of identifying and extracting relevant job information such as titles, companies, locations, and salaries."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import random"
      ],
      "metadata": {
        "id": "LkGGiM-I1IGy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dV8oeifrzJ25"
      },
      "outputs": [],
      "source": [
        "def get_job_site(url):\n",
        "  res = requests.get(url)\n",
        "  soup = BeautifulSoup(res.text, 'html.parser')\n",
        "  return soup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_job_info(job_info):\n",
        "  url_tags = job_info.select(\"a\")\n",
        "  title_tag = job_info.find(\"h2\", class_ = \"text-[18px] leading-[1.5] font-medium whitespace-wrap break-all\")\n",
        "  company_tag = job_info.find(\"h2\", class_ = \"inline\")\n",
        "  location_tag = job_info.find(\"a\", class_ = \"job-card-condition__text cursor-pointer hover:underline underline-offset-2\")\n",
        "  salary_tag = job_info.find(\"h4\", class_ = \"job-card-condition__text\")\n",
        "\n",
        "  job_url = url_tags[0][\"href\"] if url_tags and \"href\" in url_tags[0].attrs else \"\"\n",
        "  job_title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
        "  company = company_tag.get_text(strip=True) if company_tag else \"\"\n",
        "  location = location_tag.get_text(strip=True) if location_tag else \"\"\n",
        "  salary = salary_tag.get_text(strip=True) if salary_tag else \"\"\n",
        "\n",
        "  return {\n",
        "      'url': job_url,\n",
        "      'title': job_title,\n",
        "      'company': company,\n",
        "      'location': location,\n",
        "      'salary': salary\n",
        "  }"
      ],
      "metadata": {
        "id": "jXevEy2HzKq3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ceec1d4"
      },
      "source": [
        "def scrape_all_pages(start_url, max_pages):\n",
        "    all_records = []\n",
        "    page = 1\n",
        "\n",
        "    # The initial URL contains page=1.\n",
        "    # For 1111 website, 'page' parameter is always in the format 'page=X' in the URL.\n",
        "    base_url_template = start_url.replace(\"page=1\", \"page={}\")\n",
        "\n",
        "    while True:\n",
        "        current_url = base_url_template.format(page)\n",
        "\n",
        "        print(f\"Fetching page {page} from {current_url}\")\n",
        "        soup = get_job_site(current_url)\n",
        "        time.sleep(random.randint(5, 15)) # Add a random delay between 5 and 15 seconds\n",
        "\n",
        "        if not soup:\n",
        "            print(f\"Failed to retrieve content for page {page}. Stopping.\")\n",
        "            break\n",
        "\n",
        "        job_listings_on_page = soup.find_all(\"div\", class_=\"flex flex-col lg:gap-4 lg:flex-row\")\n",
        "\n",
        "        # If no job listings are found, it might indicate the end of available pages\n",
        "        if not job_listings_on_page:\n",
        "            print(f\"No job listings found on page {page}. Stopping.\")\n",
        "            break\n",
        "\n",
        "        df_page = build_records(soup)\n",
        "        if not df_page.empty:\n",
        "            all_records.append(df_page)\n",
        "\n",
        "        page += 1\n",
        "        if max_pages and page > max_pages:\n",
        "            print(f\"Reached maximum page limit ({max_pages}). Stopping.\")\n",
        "            break\n",
        "\n",
        "    if all_records:\n",
        "        full_df = pd.concat(all_records, ignore_index=True)\n",
        "        return full_df\n",
        "    else:\n",
        "        print(\"No records found after scraping.\")\n",
        "        return pd.DataFrame() # Return an empty DataFrame if no records are found"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_records(soup):\n",
        "  records = []\n",
        "  for job_info in soup.find_all(\"div\", class_=\"flex flex-col lg:gap-4 lg:flex-row\"):\n",
        "    record = get_job_info(job_info)\n",
        "    records.append(record)\n",
        "  df = pd.DataFrame(records)\n",
        "  return df"
      ],
      "metadata": {
        "id": "bSGIGJ1Xzc4N"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_file(df):\n",
        "  url_prefix = \"https://www.1111.com.tw\"\n",
        "\n",
        "  df['url'] = url_prefix + df['url']\n",
        "  df.to_excel('job_listings.xlsx', index= False)\n",
        "  print('DataFrame successfully saved to job_listings.xlsx')"
      ],
      "metadata": {
        "id": "Nq4r6fBV0Qn-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56bc3a3c",
        "outputId": "b62d7dcd-a8a1-44e7-ebda-1fc39d9d184b"
      },
      "source": [
        "# Main execution\n",
        "url = \"https://www.1111.com.tw/search/job?page=1&col=da&sort=desc&ks=%E8%9B%8B%E7%99%BD\"\n",
        "\n",
        "df_jobs = scrape_all_pages(url, max_pages=None) # Scrape all available pages\n",
        "\n",
        "if not df_jobs.empty:\n",
        "    create_file(df_jobs)\n",
        "    print(f\"Successfully processed and saved {len(df_jobs)} job listings from multiple pages.\")\n",
        "else:\n",
        "    print(\"No job listings found or processed.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching page 1 from https://www.1111.com.tw/search/job?page=1&col=da&sort=desc&ks=%E8%9B%8B%E7%99%BD\n",
            "Fetching page 2 from https://www.1111.com.tw/search/job?page=2&col=da&sort=desc&ks=%E8%9B%8B%E7%99%BD\n",
            "Fetching page 3 from https://www.1111.com.tw/search/job?page=3&col=da&sort=desc&ks=%E8%9B%8B%E7%99%BD\n",
            "Fetching page 4 from https://www.1111.com.tw/search/job?page=4&col=da&sort=desc&ks=%E8%9B%8B%E7%99%BD\n",
            "Fetching page 5 from https://www.1111.com.tw/search/job?page=5&col=da&sort=desc&ks=%E8%9B%8B%E7%99%BD\n",
            "No job listings found on page 5. Stopping.\n",
            "DataFrame successfully saved to job_listings.xlsx\n",
            "Successfully processed and saved 40 job listings from multiple pages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Q_uk7qEjzQml"
      }
    }
  ]
}